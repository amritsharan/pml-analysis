<!DOCTYPE html>
<html>
<head>
  <title>Practical Machine Learning: Weight Lifting Prediction</title>
  <style>
    body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
    h1 { color: #333; border-bottom: 3px solid #007bff; padding-bottom: 10px; }
    h2 { color: #555; margin-top: 30px; }
    table { border-collapse: collapse; width: 100%; margin: 20px 0; }
    th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }
    th { background-color: #007bff; color: white; }
    tr:nth-child(even) { background-color: #f2f2f2; }
    code { background-color: #f4f4f4; padding: 2px 4px; border-radius: 3px; }
    .highlight { background-color: #fff3cd; padding: 10px; border-left: 4px solid #ffc107; margin: 10px 0; }
  </style>
</head>
<body>

<h1>Practical Machine Learning: Weight Lifting Exercise Prediction</h1>
<p><strong>Author:</strong> AMRIT S R | <strong>Date:</strong> November 5, 2025</p>

<h2>Executive Summary</h2>
<p>This analysis predicts exercise quality using accelerometer data from 6 participants performing barbell lifts. The Random Forest model with 5-fold cross-validation achieved 99.14% accuracy with an out-of-sample error of 0.86%.</p>

<h2>Data Overview</h2>
<ul>
  <li><strong>Training data:</strong> 19,622 observations with 160 variables</li>
  <li><strong>Test data:</strong> 20 observations for prediction</li>
  <li><strong>Target variable (classe):</strong> 5 classes (A, B, C, D, E)</li>
</ul>

<h2>Data Preprocessing</h2>
<p>The following preprocessing steps were applied:</p>
<ol>
  <li>Removed columns with >95% missing values (107 columns removed)</li>
  <li>Removed non-predictive columns (X, user_name, timestamps, window info)</li>
  <li><strong>Result:</strong> 52 predictor variables retained</li>
</ol>
<table>
  <tr><th>Metric</th><th>Original</th><th>Cleaned</th></tr>
  <tr><td>Observations</td><td>19,622</td><td>19,622</td></tr>
  <tr><td>Variables</td><td>160</td><td>53</td></tr>
</table>

<h2>Data Partitioning</h2>
<ul>
  <li><strong>Training set:</strong> 13,737 observations (70%)</li>
  <li><strong>Validation set:</strong> 5,885 observations (30%)</li>
</ul>

<h2>Model Building</h2>

<h3>Cross-Validation Strategy</h3>
<p>I used <strong>5-fold cross-validation</strong> because:</p>
<ul>
  <li>Divides training data into 5 equal parts</li>
  <li>Trains 5 models, each holding out one fold for validation</li>
  <li>Provides robust error estimates reducing bias and variance</li>
  <li>Balances computational efficiency with reliability</li>
</ul>

<h3>Model Selection: Random Forest</h3>
<p>Random Forest was chosen because it:</p>
<ul>
  <li>Handles non-linear relationships effectively</li>
  <li>Provides variable importance rankings</li>
  <li>Produces excellent results for multi-class classification</li>
  <li>Is robust to overfitting through ensemble averaging</li>
</ul>

<h2>Model Evaluation</h2>

<h3>Cross-Validation Results</h3>
<table>
  <tr><th>mtry</th><th>Accuracy</th><th>Kappa</th><th>Accuracy SD</th></tr>
  <tr><td>2</td><td>0.9915</td><td>0.9892</td><td>0.0019</td></tr>
  <tr><td>27</td><td>0.9902</td><td>0.9877</td><td>0.0012</td></tr>
  <tr><td>52</td><td>0.9847</td><td>0.9807</td><td>0.0027</td></tr>
</table>
<p><strong>Best tuning parameter selected: mtry = 2</strong></p>

<h3>Validation Set Performance</h3>
<div class="highlight">
  <strong>Overall Accuracy: 99.14%</strong><br>
  <strong>Out-of-Sample Error: 0.86%</strong>
</div>

<h3>Confusion Matrix Summary</h3>
<table>
  <tr><th>Class</th><th>Sensitivity</th><th>Specificity</th><th>Pos Pred Value</th></tr>
  <tr><td>A</td><td>0.9957</td><td>0.9988</td><td>0.9974</td></tr>
  <tr><td>B</td><td>0.9902</td><td>0.9958</td><td>0.9900</td></tr>
  <tr><td>C</td><td>0.9896</td><td>0.9972</td><td>0.9898</td></tr>
  <tr><td>D</td><td>0.9879</td><td>0.9979</td><td>0.9930</td></tr>
  <tr><td>E</td><td>0.9949</td><td>0.9993</td><td>0.9955</td></tr>
</table>

<h2>Final Predictions (20 Test Cases)</h2>
<table>
  <tr><th>Problem ID</th><th>Predicted Class</th></tr>
  <tr><td>1</td><td>B</td></tr>
  <tr><td>2</td><td>A</td></tr>
  <tr><td>3</td><td>B</td></tr>
  <tr><td>4</td><td>A</td></tr>
  <tr><td>5</td><td>A</td></tr>
  <tr><td>6</td><td>E</td></tr>
  <tr><td>7</td><td>D</td></tr>
  <tr><td>8</td><td>B</td></tr>
  <tr><td>9</td><td>A</td></tr>
  <tr><td>10</td><td>A</td></tr>
  <tr><td>11</td><td>B</td></tr>
  <tr><td>12</td><td>C</td></tr>
  <tr><td>13</td><td>B</td></tr>
  <tr><td>14</td><td>A</td></tr>
  <tr><td>15</td><td>E</td></tr>
  <tr><td>16</td><td>E</td></tr>
  <tr><td>17</td><td>A</td></tr>
  <tr><td>18</td><td>B</td></tr>
  <tr><td>19</td><td>B</td></tr>
  <tr><td>20</td><td>B</td></tr>
</table>

<h2>Model Selection Justification</h2>
<ol>
  <li><strong>Random Forest over other methods:</strong> Tested mtry values of 2, 27, and 52. mtry=2 achieved 99.14% accuracy.</li>
  <li><strong>5-fold cross-validation:</strong> Balances bias-variance tradeoff. Consistent accuracy across folds (SD=0.19%).</li>
  <li><strong>Out-of-sample error:</strong> Calculated from independent validation set (0.86%), providing unbiased estimate.</li>
</ol>

<h2>Conclusion</h2>
<p>The Random Forest model achieved <strong>99.14% accuracy</strong> with an <strong>out-of-sample error of 0.86%</strong> on the validation set. This exceptional performance demonstrates the model's ability to correctly classify exercise quality based on accelerometer sensor data. The model is ready for deployment.</p>

<hr>
<p><em>Data source: Groupware@LES HAR Dataset - Velloso, E., Bulling, A., Gellersen, H., Ugulino, W., & Fuentes, H. (2013)</em></p>

</body>
</html>